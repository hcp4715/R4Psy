
#加载包
```{r}
pacman::p_load(
  psych, lavaan, ggplot2, reshape2, caret,
  FactoMineR, factoextra, tidyverse, linkET,
  vegan, ggnewscale, RColorBrewer, EGAnet, xgboost,
  semPlot, randomForest, Metrics , qgraph, reader, dplyr,
  tidyselect, corrplot, dendextend
)
```

#数据处理
```{r}

data <- read.csv("./Complete_02-16-2019/meaningful_variables.csv") # 使用相对路径读取数据
# 选择以指定前缀开头的列为survey
prefixes <- c("bis", "brief", "dickman", "dospert", "eating", "erq", "five", "future", "grit", "impulsive_venture_survey.venturesomeness",
              "mindful", "mph", "selection", "self", "sensation", "ten", "theories", "time", "upps")
survey <- data %>%
  select(starts_with(prefixes))
# 提取剩余的列并命名为task
task <- data %>%
  select(-starts_with(prefixes))
columns_to_remove <- c("angling_risk_task_always_sunny.keep_score", "angling_risk_task_always_sunny.release_score",
                       "holt_laury_survey.prob_weighting", "impulsive_venture_survey.impulsiveness", "kirby.hyp_discount_rate_medium",
                       "kirby.percent_patient","kirby.percent_patient_large", "kirby.percent_patient_medium",
                       "kirby.percent_patient_small", "mpq_control_survey.control", "probabilistic_selection.value_sensitivity", "X")
task <- task %>%
  select(-columns_to_remove)
# 查看以确认结果
head(task)
head(survey)
```

#探索性因素分析
```{r}
# 对调查结果进行EFA分析
survey <- as.data.frame(survey) # 确保 survey 是数据框
survey <- data.frame(lapply(survey, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))) # 处理缺失值，使用列均值进行替换
survey <- data.frame(lapply(survey, as.numeric)) # 确保所有变量都是数值型
cormatrix <- cor(survey, use = "complete.obs") # 计算相关矩阵

# 根据贝叶斯信息准则（BIC）确认最佳因子数（11）
fa_result_survey <- list()
bic_values_survey <- c()
for (i in 1:15) {  # 测试从 1 到 15 个因子
  fit <- fa(survey, nfactors = i, fm = "minres")
  fa_result_survey[[i]] <- fit
  bic_values_survey[i] <- fit$BIC
}
print(bic_values_survey)
best_factors_survey <- which.min(bic_values_survey)
print(best_factors_survey)  # 输出最佳因子数

# 基于最佳因子数进行探索性因子分析
efa_result_survey1 <- fa(survey, nfactors = best_nfactors_parallel_survey, rotate = "oblimin", fm = "minres")
print(efa_result_survey)
# 可视化
fa.diagram(efa_result_survey1, simple = T) # simple = FALSE会显示所有载荷，如果设置为TRUE则只显示显著的载荷

# 对认知任务进行EFA分析
task <- as.data.frame(task)
task <- data.frame(lapply(task, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))) 
task <- data.frame(lapply(task, as.numeric)) 
cormatrix <- cor(task, use = "complete.obs") 
# 根据贝叶斯信息准则（BIC）确认最佳因子数（5）
fa_result_task <- list()
bic_values_task <- c()
for (i in 1:10) {  # 测试从 1 到 10 个因子
  fit <- fa(task, nfactors = i, fm = "minres")
  fa_result_task[[i]] <- fit
  bic_values_task[i] <- fit$BIC
}
print(bic_values_task)
best_factors_task <- which.min(bic_values_task)
print(best_factors_task)  # 输出最佳因子数

efa_result_task <- fa(task, nfactors = best_factors_task, rotate = "oblimin", fm = "minres")
fa.diagram(efa_result_task, simple = T)
print(efa_result_task)

```

#评估调查与任务之间的关联

##survey相关
```{r}
survey_z <- scale(survey)
cor_matrix_survey <- cor(survey_z, method = "pearson")  # 可选方法：pearson/spearman/kendall
corrplot(
  cor_matrix_survey, 
  method = "color", 
  type = "upper", 
  tl.cex = 0.01,
 tl.col = "black", 
 tl.srt = 45,
 diag = FALSE,
  col = colorRampPalette(c("blue", "white", "red"))(20),
  title = "The correlation of survey"
)
```
#task之间的相关
```{r}
task_z <- scale(task)
cor_matrix_task <- cor(task_z, method = "pearson")  # 可选方法：pearson/spearman/kendall
corrplot(
  cor_matrix_task, 
  method = "color", 
  type = "upper", 
  tl.cex = 0.01,
 tl.col = "black", 
 tl.srt = 45,
 diag = FALSE,
  col = colorRampPalette(c("blue", "white", "red"))(20),
  title = "The correlation of task"
)
```

#网络图
```{r}
survey_scaled <- scale(survey)  # 标准化
task_scaled <- scale(task)      # 标准化

# 计算相关性矩阵
corr_survey <- cor(survey_scaled, use = "pairwise.complete.obs")
corr_task <- cor(task_scaled, use = "pairwise.complete.obs")
corr_between <- cor(survey_scaled, task_scaled, use = "pairwise.complete.obs")  # survey 和 task 之间的相关性

# 构建完整的相关性矩阵
combined_matrix <- matrix(0, nrow = ncol(survey) + ncol(task), ncol = ncol(survey) + ncol(task))
rownames(combined_matrix) <- c(colnames(survey), colnames(task))
colnames(combined_matrix) <- c(colnames(survey), colnames(task))

# 填充相关性矩阵
combined_matrix[1:ncol(survey), 1:ncol(survey)] <- corr_survey
combined_matrix[(ncol(survey) + 1):nrow(combined_matrix), (ncol(survey) + 1):ncol(combined_matrix)] <- corr_task
combined_matrix[1:ncol(survey), (ncol(survey) + 1):ncol(combined_matrix)] <- corr_between
combined_matrix[(ncol(survey) + 1):nrow(combined_matrix), 1:ncol(survey)] <- t(corr_between)

# 定义分组
groups <- list(
  "Survey" = 1:ncol(survey),
  "Task" = (ncol(survey) + 1):ncol(combined_matrix)
)

# 绘制网络图
qgraph(
  combined_matrix,
  groups = groups,
  layout = "spring",        # 自动布局
  labels = colnames(combined_matrix),
  legend = TRUE,
  label.cex = 0.7,          # 标签字体大小
  vsize = 5,                # 节点大小
  curveAll = F,          # 使用曲线而不是直线
  edge.color = "black", 
  esize = 1.2,              # 边厚度
  color = c("#5d90ba", "#bd6263"),  # 节点颜色：Survey 蓝色，Task 橙色
  title = "Network Analysis: Survey and Task",
  legend.cex = 0.8          # 图例大小
)

```
##保留一个变量时其他变量的预测程度（线性）
```{r}
# 合并survey和task
merged_data <- cbind(survey, task)

# 获取所有变量名
variables <- names(merged_data)
# 初始化一个列表来保存模型结果
model_results <- list()
# 设置交叉验证的参数
control <- trainControl(method = "cv", number = 10) # 10折交叉验证
# 循环遍历每个变量
for(target_var in variables) {
# 排除当前目标变量，获取预测变量
  predictors <- setdiff(variables, target_var)
# 创建公式
  formula <- as.formula(paste(target_var, "~", paste(predictors, collapse = "+")))
# 训练模型，这里使用线性回归作为示例，你可以根据需要更换为其他模型
  model <- train(formula, data = merged_data, method = "lm", trControl = control)
# 保存模型结果
  model_results[[target_var]] <- model
}
# 输出每个模型的性能
for(target_var in names(model_results)) 
  cat("Model performance for", target_var, ":\n")
  print(model_results[[target_var]])
# 初始化一个数据框来保存所有模型的性能指标
performance_df <- data.frame(Target_Variable = character(),
                             RMSE = numeric(),
                             Rsquared = numeric(),
                             MAE = numeric(),
                             stringsAsFactors = FALSE)
# 提取每个模型的性能指标
for(target_var in names(model_results)) {
  # 提取性能指标
  results <- model_results[[target_var]]$results
  rmse <- ifelse(exists("RMSE", results), results$RMSE, NA)
  rsquared <- ifelse(exists("Rsquared", results), results$Rsquared, NA)
  mae <- ifelse(exists("MAE", results), results$MAE, NA)
# 将性能指标添加到数据框
  performance_df <- rbind(performance_df, data.frame(Target_Variable = target_var,
                                                     RMSE = rmse,
                                                     Rsquared = rsquared,
                                                     MAE = mae,
                                                     stringsAsFactors = FALSE))
}

# 将数据框写入 CSV 文件
#write.csv(performance_df, "model_performance.csv", row.names = FALSE)
# 加载ggplot2包
library(ggplot2)

# 读取之前保存的性能指标数据框
#performance_df <- read.csv("model_performance.csv")

# 创建RMSE的条形图
ggplot(performance_df, aes(x = reorder(Target_Variable, RMSE), y = RMSE)) +
  geom_bar(stat = "identity") +
  coord_flip() + # 翻转坐标轴，使得变量名更容易阅读
  labs(x = "Target Variable", y = "RMSE", title = "Root Mean Squared Error for Each Model") +
  theme_minimal()

# 创建R-squared的条形图（0.5）
ggplot(performance_df, aes(x = reorder(Target_Variable, Rsquared), y = Rsquared)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Target Variable", y = "R-squared", title = "R-squared for Each Model") +
  theme_minimal()

# 创建MAE的条形图
ggplot(performance_df, aes(x = reorder(Target_Variable, MAE), y = MAE)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Target Variable", y = "MAE", title = "Mean Absolute Error for Each Model") +
  theme_minimal()
```


#聚类分析
```{r}
#对survey因子载荷进行聚类分析（9）
library(dynamicTreeCut)

loadings1 <- efa_result_survey$loadings
distances <- dist(loadings1, method="euclidean")
 dendro <- hclust(distances, method="complete")
 clusters <- cutreeDynamic(dendro, method="hybrid",minClusterSize=4,  distM=as.matrix(distances),deepSplit=4, maxCoreScatter=NULL, minGap=NULL, maxAbsCoreScatter=NULL, minAbsGap=NULL)
 print(table(clusters))
# 将层次聚类树转换为树状图
dendrogram <- as.dendrogram(dendro)
# 检查树状图对象
print(dendrogram)
# 使用colorbranches函数将树状图的颜色设置为与聚类数相匹配的数量
coloreddendrogram <- color_branches(dendrogram, k = max(clusters))
# 绘制树状图
plot(coloreddendrogram)
```



```{r}
# 对survey进行bifactor分析
# 自动确定模型因子数量（11）
fa_parallel <- fa.parallel(survey, fa = "fa", fm = "minres", n.iter = 100)
nfactors <- fa_parallel$nfact  # 推荐因子数量
# 进行 EFA
efa_result1 <- fa(r = cor(survey), nfactors = nfactors, fm = "minres", rotate = "oblimin")
# 打印因子分析结果，查看因子载荷
print(efa_result1)
# 提取因子载荷矩阵
loadings <- as.data.frame(efa_result1$loadings)
# 根据因子载荷动态生成 bifactor 模型公式
threshold <- 0.3  # 因子载荷的阈值
model_formula <- ""

for (factor_idx in 1:nfactors) {
  # 检查因子索引是否在范围内
  if (factor_idx <= ncol(loadings$x)) {
    # 提取当前因子相关的条目
    factor_items <- rownames(loadings$x)[abs(loadings$x[, factor_idx]) > threshold]
    factor_name <- paste0("F", factor_idx)
    
    # 为模型公式添加条目
    if (length(factor_items) > 0) {
      model_formula <- paste0(
        model_formula,
        factor_name, " =~ ", paste(factor_items, collapse = " + "), "\n"
      )
    }
  }
}

# 添加通用因子
general_factor_items <- rownames(loadings$x)
model_formula <- paste0(
  "I =~ ", paste(general_factor_items, collapse = " + "), "\n", 
  model_formula
)
# 打印生成的模型公式
print(model_formula)
```

```{r}
# 模型
model_formula <- '
  # 总体因子
  I =~ bis11_survey.Attentional + bis11_survey.Motor + bis11_survey.Nonplanning + bis_bas_survey.BAS_drive + bis_bas_survey.BAS_fun_seeking + bis_bas_survey.BAS_reward_responsiveness + bis_bas_survey.BIS + brief_self_control_survey.self_control + dickman_survey.dysfunctional + dickman_survey.functional + dospert_eb_survey.ethical + dospert_eb_survey.financial + dospert_eb_survey.health_safety + dospert_eb_survey.recreational + dospert_eb_survey.social + dospert_rp_survey.ethical + dospert_rp_survey.financial + dospert_rp_survey.health_safety + dospert_rp_survey.recreational + dospert_rp_survey.social + dospert_rt_survey.ethical + dospert_rt_survey.financial + dospert_rt_survey.health_safety + dospert_rt_survey.recreational + dospert_rt_survey.social + eating_survey.cognitive_restraint + eating_survey.emotional_eating + eating_survey.uncontrolled_eating + erq_survey.reappraisal + erq_survey.suppression + five_facet_mindfulness_survey.act_with_awareness + five_facet_mindfulness_survey.describe + five_facet_mindfulness_survey.nonjudge + five_facet_mindfulness_survey.nonreact + five_facet_mindfulness_survey.observe + future_time_perspective_survey.future_time_perspective + grit_scale_survey.grit + impulsive_venture_survey.venturesomeness + mindful_attention_awareness_survey.mindfulness + selection_optimization_compensation_survey.compensation + selection_optimization_compensation_survey.elective_selection + selection_optimization_compensation_survey.loss_based_selection + selection_optimization_compensation_survey.optimization + self_regulation_survey.control + sensation_seeking_survey.boredom_susceptibility + sensation_seeking_survey.disinhibition + sensation_seeking_survey.experience_seeking + sensation_seeking_survey.thrill_adventure_seeking + ten_item_personality_survey.agreeableness + ten_item_personality_survey.conscientiousness + ten_item_personality_survey.emotional_stability + ten_item_personality_survey.extraversion + ten_item_personality_survey.openness + theories_of_willpower_survey.endorse_limited_resource + time_perspective_survey.future + time_perspective_survey.past_negative + time_perspective_survey.past_positive + time_perspective_survey.present_fatalistic + time_perspective_survey.present_hedonistic + upps_impulsivity_survey.lack_of_perseverance + upps_impulsivity_survey.lack_of_premeditation + upps_impulsivity_survey.negative_urgency + upps_impulsivity_survey.positive_urgency + upps_impulsivity_survey.sensation_seeking
  
  # 特定因子
F1 =~ bis_bas_survey.BAS_fun_seeking + dospert_eb_survey.recreational + dospert_rp_survey.recreational + dospert_rt_survey.recreational + impulsive_venture_survey.venturesomeness + sensation_seeking_survey.thrill_adventure_seeking + upps_impulsivity_survey.sensation_seeking
F2 =~ bis11_survey.Motor + bis11_survey.Nonplanning + brief_self_control_survey.self_control + dickman_survey.dysfunctional + self_regulation_survey.control + ten_item_personality_survey.conscientiousness + time_perspective_survey.future + time_perspective_survey.present_fatalistic + time_perspective_survey.present_hedonistic + upps_impulsivity_survey.lack_of_perseverance + upps_impulsivity_survey.lack_of_premeditation + upps_impulsivity_survey.negative_urgency + upps_impulsivity_survey.positive_urgency
F3 =~ bis11_survey.Attentional + bis_bas_survey.BIS + erq_survey.reappraisal + five_facet_mindfulness_survey.nonjudge + five_facet_mindfulness_survey.nonreact + future_time_perspective_survey.future_time_perspective + self_regulation_survey.control + ten_item_personality_survey.emotional_stability + time_perspective_survey.past_negative + upps_impulsivity_survey.negative_urgency
F4 =~ bis_bas_survey.BAS_drive + grit_scale_survey.grit + selection_optimization_compensation_survey.compensation + selection_optimization_compensation_survey.elective_selection + selection_optimization_compensation_survey.loss_based_selection + selection_optimization_compensation_survey.optimization + self_regulation_survey.control + time_perspective_survey.future + upps_impulsivity_survey.lack_of_perseverance
F5 =~ bis_bas_survey.BAS_drive + bis_bas_survey.BAS_fun_seeking + bis_bas_survey.BAS_reward_responsiveness + dickman_survey.functional + sensation_seeking_survey.boredom_susceptibility + ten_item_personality_survey.extraversion + ten_item_personality_survey.openness + time_perspective_survey.present_hedonistic
F6 =~ brief_self_control_survey.self_control + eating_survey.emotional_eating + eating_survey.uncontrolled_eating + five_facet_mindfulness_survey.act_with_awareness + mindful_attention_awareness_survey.mindfulness + upps_impulsivity_survey.negative_urgency
F7 =~ dospert_eb_survey.ethical + dospert_eb_survey.health_safety + dospert_rt_survey.ethical + dospert_rt_survey.health_safety
F8 =~ dospert_rp_survey.ethical + dospert_rp_survey.financial + dospert_rp_survey.health_safety + dospert_rp_survey.recreational + dospert_rp_survey.social
F9 =~ erq_survey.suppression + mindful_attention_awareness_survey.mindfulness + sensation_seeking_survey.boredom_susceptibility + ten_item_personality_survey.agreeableness + time_perspective_survey.past_positive
F10 =~ dospert_eb_survey.social + dospert_rp_survey.social + dospert_rt_survey.social + sensation_seeking_survey.experience_seeking\n
F11 =~ dospert_eb_survey.financial + dospert_rp_survey.financial + dospert_rt_survey.financial

  # 因子之间的协方差
  #F1 ~~ F2 + F3
  #F4 ~~ F5 + F6
  #F7 ~~ F8
'
```
```{r}
# 拟合模型
model_formula = lavaan::sem(model_formula, 
                      data=survey, 
                      #ordered = colnames(data_z), 
                      std.lv = TRUE, 
                      fixed.x = F,
                      orthogonal = TRUE)
# 输出模型拟合结果和标准化估计
summary(model_formula , standardized = TRUE, fit.measures = TRUE)
semPaths(model_formula , what = 'std', fade = F)

```
```{r}
#summary(model_formula, standardized = TRUE, fit.measures = TRUE)

# 提取因子得分
#factor_scores1 <- predict(model_formula)  # `type = "factor.scores"` 不是 lavaan 支持的参数，直接用 predict 提取
head(factor_scores1)  # 查看前几行因子得分

# 提取因子载荷矩阵
#standardized_solution <- parameterEstimates(model_formula, standardized = TRUE)
#factor_loadings1 <- subset(standardized_solution, op == "=~")  # 提取因子载荷
#head(factor_loadings1)  # 查看前几行因子载荷

# 可视化路径图
#library(semPlot)
#semPaths(fit, 
         #what = "std",  # 标准化路径系数
         #fade = FALSE,  # 不淡化路径
         #layout = "tree",  # 树形布局
         #style = "lisrel",  # LISREL 风格
         #residuals = TRUE,  # 显示残差
         #title = TRUE,  # 显示标题
         #label.cex = 0.8,  # 调整路径标签大小
         #edge.label.cex = 0.8,  # 调整路径系数大小
         #nCharNodes = 0,  # 隐藏节点标签
         #sizeMan = 8,  # 调整观测变量节点大小
         #sizeLat = 10,  # 调整潜变量节点大小
         #mar = c(5, 5, 5, 5))  # 调整边距
```

#聚类分析
```{r}
#对task因子载荷进行聚类分析（20）
loadings2 <- efa_result_task$loadings
distances <- dist(loadings2, method="euclidean")
 dendro <- hclust(distances, method="complete")
 clusters <- cutreeDynamic(dendro, method="hybrid",minClusterSize=4,  distM=as.matrix(distances),deepSplit=4, maxCoreScatter=NULL, minGap=NULL, maxAbsCoreScatter=NULL, minAbsGap=NULL)
 print(table(clusters))
# 将层次聚类树转换为树状图
dendrogram <- as.dendrogram(dendro)
# 检查树状图对象
print(dendrogram)
# 使用colorbranches函数将树状图的颜色设置为与聚类数相匹配的数量
coloreddendrogram <- color_branches(dendrogram, k = max(clusters))
# 绘制树状图
plot(coloreddendrogram)
```

```{r}
# 对task进行bifactor分析
# 自动确定模型因子数量
fa_parallel <- fa.parallel(task, fa = "fa", fm = "minres", n.iter = 100)
nfactors <- fa_parallel$nfact  # 推荐因子数量
# 进行 EFA
efa_result2 <- fa(r = cor(task), nfactors = nfactors, fm = "minres", rotate = "oblimin")
# 打印因子分析结果，查看因子载荷
print(efa_result2)
# 提取因子载荷矩阵
loadings <- as.data.frame(efa_result2$loadings)
# 根据因子载荷动态生成 bifactor 模型公式
threshold <- 0.3  # 因子载荷的阈值
model_formula <- ""

for (factor_idx in 1:nfactors) {
  # 检查因子索引是否在范围内
  if (factor_idx <= ncol(loadings$x)) {
    # 提取当前因子相关的条目
    factor_items <- rownames(loadings$x)[abs(loadings$x[, factor_idx]) > threshold]
    factor_name <- paste0("F", factor_idx)
    
    # 为模型公式添加条目
    if (length(factor_items) > 0) {
      model_formula <- paste0(
        model_formula,
        factor_name, " =~ ", paste(factor_items, collapse = " + "), "\n"
      )
    }
  }
}

# 添加通用因子
general_factor_items <- rownames(loadings$x)
model_formula <- paste0(
  "I =~ ", paste(general_factor_items, collapse = " + "), "\n", 
  model_formula
)
# 打印生成的模型公式
print(model_formula)
```

```{r}
# 模型
model_formula <- '
  # 总体因子
I =~ adaptive_n_back.hddm_drift + adaptive_n_back.hddm_drift_load + adaptive_n_back.hddm_non_decision + adaptive_n_back.hddm_thresh + adaptive_n_back.mean_load + angling_risk_task_always_sunny.keep_adjusted_clicks + angling_risk_task_always_sunny.keep_coef_of_variation + angling_risk_task_always_sunny.release_adjusted_clicks + angling_risk_task_always_sunny.release_coef_of_variation + attention_network_task.alerting_hddm_drift + attention_network_task.conflict_hddm_drift + attention_network_task.hddm_drift + attention_network_task.hddm_non_decision + attention_network_task.hddm_thresh + attention_network_task.orienting_hddm_drift + bickel_titrator.hyp_discount_rate_large + bickel_titrator.hyp_discount_rate_medium + bickel_titrator.hyp_discount_rate_small + choice_reaction_time.hddm_drift + choice_reaction_time.hddm_non_decision + choice_reaction_time.hddm_thresh + cognitive_reflection_survey.correct_proportion + cognitive_reflection_survey.intuitive_proportion + columbia_card_task_cold.avg_cards_chosen + columbia_card_task_cold.gain_sensitivity + columbia_card_task_cold.information_use + columbia_card_task_cold.loss_sensitivity + columbia_card_task_cold.probability_sensitivity + columbia_card_task_hot.avg_cards_chosen + columbia_card_task_hot.gain_sensitivity + columbia_card_task_hot.information_use + columbia_card_task_hot.loss_sensitivity + columbia_card_task_hot.probability_sensitivity + dietary_decision.health_sensitivity + dietary_decision.taste_sensitivity + digit_span.forward_span + digit_span.reverse_span + directed_forgetting.hddm_drift + directed_forgetting.hddm_non_decision + directed_forgetting.hddm_thresh + directed_forgetting.proactive_interference_hddm_drift + discount_titrate.percent_patient + dot_pattern_expectancy.AY.BY_hddm_drift + dot_pattern_expectancy.BX.BY_hddm_drift + dot_pattern_expectancy.bias + dot_pattern_expectancy.dprime + dot_pattern_expectancy.hddm_drift + dot_pattern_expectancy.hddm_non_decision + dot_pattern_expectancy.hddm_thresh + go_nogo.bias + go_nogo.dprime + hierarchical_rule.score + holt_laury_survey.beta + holt_laury_survey.risk_aversion + holt_laury_survey.safe_choices + information_sampling_task.Decreasing_Win_P_correct + information_sampling_task.Decreasing_Win_motivation + information_sampling_task.Fixed_Win_P_correct + information_sampling_task.Fixed_Win_motivation + keep_track.score + kirby.hyp_discount_rate_large + kirby.hyp_discount_rate_small + local_global_letter.conflict_hddm_drift + local_global_letter.global_bias_hddm_drift + local_global_letter.hddm_drift + local_global_letter.hddm_non_decision + local_global_letter.hddm_thresh + local_global_letter.switch_cost_hddm_drift + motor_selective_stop_signal.SSRT + motor_selective_stop_signal.hddm_drift + motor_selective_stop_signal.hddm_non_decision + motor_selective_stop_signal.hddm_thresh + motor_selective_stop_signal.proactive_control_hddm_drift + motor_selective_stop_signal.reactive_control_hddm_drift + probabilistic_selection.positive_learning_bias + psychological_refractory_period_two_choices.PRP_slope + ravens.score + recent_probes.hddm_drift + recent_probes.hddm_non_decision + recent_probes.hddm_thresh + recent_probes.proactive_interference_hddm_drift + shape_matching.hddm_drift + shape_matching.hddm_non_decision + shape_matching.hddm_thresh + shape_matching.stimulus_interference_hddm_drift + shift_task.acc + shift_task.learning_rate + shift_task.learning_to_learn + shift_task.model_beta + shift_task.model_decay + shift_task.model_learning_rate + simon.hddm_drift + simon.hddm_non_decision + simon.hddm_thresh + simon.simon_hddm_drift + simple_reaction_time.avg_rt + spatial_span.forward_span + spatial_span.reverse_span + stim_selective_stop_signal.SSRT + stim_selective_stop_signal.hddm_drift + stim_selective_stop_signal.hddm_non_decision + stim_selective_stop_signal.hddm_thresh + stim_selective_stop_signal.reactive_control_hddm_drift + stop_signal.SSRT_high + stop_signal.SSRT_low + stop_signal.hddm_drift + stop_signal.hddm_non_decision + stop_signal.hddm_thresh + stop_signal.proactive_SSRT_speeding + stop_signal.proactive_slowing_hddm_drift + stop_signal.proactive_slowing_hddm_thresh + stroop.hddm_drift + stroop.hddm_non_decision + stroop.hddm_thresh + stroop.stroop_hddm_drift + threebytwo.cue_switch_cost_hddm_drift + threebytwo.hddm_drift + threebytwo.hddm_non_decision + threebytwo.hddm_thresh + threebytwo.task_switch_cost_hddm_drift + tower_of_london.avg_move_time + tower_of_london.num_extra_moves + tower_of_london.num_optimal_solutions + tower_of_london.planning_time + two_stage_decision.model_based + two_stage_decision.model_free + two_stage_decision.perseverance + writing_task.neutral_probability + writing_task.positive_probability

 # 特定因子
F1 =~ dot_pattern_expectancy.BX.BY_hddm_drift + dot_pattern_expectancy.bias + dot_pattern_expectancy.dprime + dot_pattern_expectancy.hddm_drift + dot_pattern_expectancy.hddm_thresh
F2 =~ columbia_card_task_cold.gain_sensitivity + columbia_card_task_cold.information_use + columbia_card_task_cold.loss_sensitivity + columbia_card_task_hot.gain_sensitivity + columbia_card_task_hot.information_use + columbia_card_task_hot.loss_sensitivity + columbia_card_task_hot.probability_sensitivity
F3 =~ choice_reaction_time.hddm_drift + go_nogo.dprime + shape_matching.hddm_drift + shape_matching.stimulus_interference_hddm_drift + simon.hddm_drift + stop_signal.hddm_drift + stroop.hddm_drift + stroop.hddm_thresh
F4 =~ adaptive_n_back.mean_load + digit_span.forward_span + digit_span.reverse_span + directed_forgetting.hddm_drift + directed_forgetting.hddm_thresh + keep_track.score + ravens.score + recent_probes.hddm_drift + spatial_span.forward_span + spatial_span.reverse_span
F5 =~ motor_selective_stop_signal.SSRT + motor_selective_stop_signal.hddm_drift + motor_selective_stop_signal.hddm_thresh + motor_selective_stop_signal.proactive_control_hddm_drift + stim_selective_stop_signal.hddm_drift + stim_selective_stop_signal.hddm_thresh + stop_signal.hddm_thresh
F6 =~ motor_selective_stop_signal.SSRT + simple_reaction_time.avg_rt + stop_signal.SSRT_high + stop_signal.SSRT_low
F7 =~ bickel_titrator.hyp_discount_rate_large + bickel_titrator.hyp_discount_rate_medium + bickel_titrator.hyp_discount_rate_small + kirby.hyp_discount_rate_large + kirby.hyp_discount_rate_small
F8 =~ columbia_card_task_cold.avg_cards_chosen + information_sampling_task.Decreasing_Win_motivation + information_sampling_task.Fixed_Win_motivation + spatial_span.forward_span + tower_of_london.avg_move_time
F9 =~ cognitive_reflection_survey.correct_proportion + cognitive_reflection_survey.intuitive_proportion
F10 =~ attention_network_task.alerting_hddm_drift + attention_network_task.hddm_drift + attention_network_task.orienting_hddm_drift
F11 =~ attention_network_task.conflict_hddm_drift + attention_network_task.hddm_drift + attention_network_task.hddm_non_decision + attention_network_task.hddm_thresh + choice_reaction_time.hddm_thresh
F12 =~ two_stage_decision.model_based + two_stage_decision.model_free + two_stage_decision.perseverance
F13 =~ tower_of_london.avg_move_time + tower_of_london.num_extra_moves + tower_of_london.planning_time
F14 =~ simon.hddm_thresh + threebytwo.cue_switch_cost_hddm_drift + threebytwo.hddm_non_decision + threebytwo.hddm_thresh + threebytwo.task_switch_cost_hddm_drift
F15 =~ angling_risk_task_always_sunny.keep_adjusted_clicks + angling_risk_task_always_sunny.keep_coef_of_variation + angling_risk_task_always_sunny.release_adjusted_clicks + angling_risk_task_always_sunny.release_coef_of_variation + columbia_card_task_hot.avg_cards_chosen
F16 =~ psychological_refractory_period_two_choices.PRP_slope + shape_matching.hddm_thresh
F17 =~ simon.hddm_non_decision + stroop.hddm_non_decision
F18 =~ adaptive_n_back.hddm_drift + adaptive_n_back.hddm_drift_load
F19 =~ stim_selective_stop_signal.hddm_non_decision + stim_selective_stop_signal.hddm_thresh + stim_selective_stop_signal.reactive_control_hddm_drift
F20 =~ shift_task.acc + shift_task.learning_rate + shift_task.model_beta + shift_task.model_learning_rate
F21 =~ recent_probes.hddm_thresh + stop_signal.hddm_non_decision + stop_signal.hddm_thresh
F22 =~ stop_signal.proactive_slowing_hddm_drift + stop_signal.proactive_slowing_hddm_thresh
F23 =~ adaptive_n_back.hddm_non_decision + adaptive_n_back.hddm_thresh
F24 =~ stop_signal.SSRT_low + stop_signal.proactive_SSRT_speeding
F25 =~ holt_laury_survey.risk_aversion + holt_laury_survey.safe_choices
F26 =~ dot_pattern_expectancy.AY.BY_hddm_drift + dot_pattern_expectancy.BX.BY_hddm_drift
'
```

```{r}
# 拟合模型
model_formula = lavaan::sem(model_formula, 
                      data=task, 
                      #ordered = colnames(task), 
                      std.lv = TRUE, 
                      fixed.x = F,
                      orthogonal = TRUE)
# 输出模型拟合结果和标准化估计
summary(model_formula , standardized = TRUE, fit.measures = TRUE)
semPaths(model_formula , what = 'std', fade = F)
```

```{r}
summary(fit, standardized = TRUE, fit.measures = TRUE)

# 提取因子得分
factor_scores2 <- predict(fit)  # `type = "factor.scores"` 不是 lavaan 支持的参数，直接用 predict 提取
head(factor_scores2)  # 查看前几行因子得分

# 提取因子载荷矩阵
standardized_solution <- parameterEstimates(fit, standardized = TRUE)
factor_loadings2 <- subset(standardized_solution, op == "=~")  # 提取因子载荷
head(factor_loadings2)  # 查看前几行因子载荷

# 可视化路径图
semPaths(fit, 
         what = "std",  # 标准化路径系数
         fade = FALSE,  # 不淡化路径
         layout = "tree",  # 树形布局
         style = "lisrel",  # LISREL 风格
         residuals = TRUE,  # 显示残差
         title = TRUE,  # 显示标题
         label.cex = 0.8,  # 调整路径标签大小
         edge.label.cex = 0.8,  # 调整路径系数大小
         nCharNodes = 0,  # 隐藏节点标签
         sizeMan = 8,  # 调整观测变量节点大小
         sizeLat = 10,  # 调整潜变量节点大小
         mar = c(5, 5, 5, 5))  # 调整边距
```

#预测分析
```{r}
# 数据处理，对现实世界的结果进行降维
## 药物和酒精使用
# 读取CSV文件
file_path1 <- "/Users/sunxinru/Desktop/osfstorage-archive (1) 2/Complete_02-16-2019/alcohol_drugs_ordinal.csv"
file_path2 <- "/Users/sunxinru/Desktop/osfstorage-archive (1) 2/Complete_02-16-2019/demographics_ordinal.csv"
file_path3 <- "/Users/sunxinru/Desktop/osfstorage-archive (1) 2/Complete_02-16-2019/health_ordinal.csv"
drugs <- read.csv(file_path1)
demo <- read.csv(file_path2)
health <- read.csv(file_path3)
# 合并数据框
merged_data <- drugs %>%
  left_join(demo, by = "X") %>%
  left_join(health, by = "X")%>%
select(-1) # 删除第一列
merged_data <- scale(merged_data)

# 根据贝叶斯信息准则（BIC）确认最佳因子数（10）
fa_result_merged <- list()
bic_values_merged <- c()
for (i in 1:15) {  # 测试从 1 到 15 个因子
  fit <- fa(merged_data, nfactors = i, fm = "minres")
  fa_result_merged[[i]] <- fit
  bic_values_merged[i] <- fit$BIC
}
print(bic_values_merged)
best_factors_merged <- which.min(bic_values_merged)
print(best_factors_merged)  # 输出最佳因子数


# 基于最佳因子数进行探索性因子分析
efa_result_merged <- fa(merged_data, nfactors = best_factors_merged, rotate = "oblimin", fm = "minres")
# 可视化
fa.diagram(efa_result_merged, simple = T) # simple = FALSE会显示所有载荷，如果设置为TRUE则只显示显著的载荷
```

#survey
```{r}
library(glmnet)
# 提取因子得分
factor_scores_survey1 <- as.data.frame(efa_result_survey1$scores)
factor_scores_merged <- as.data.frame(efa_result_merged$scores)
# 添加到原始数据框
merged_with_scores <- cbind(factor_scores_survey1 , factor_scores_merged)
merged_with_scores <- na.omit(merged_with_scores)

# 提取因子数据 (F1 ~ F12) 和健康指标 (第15 ~ 18列)
X <- as.matrix(merged_with_scores[, 1:11])  # 因子数据
y <- as.matrix(merged_with_scores[, 12:21])  # 健康指标

# 设置预测目标
target <- y[, 1]

# 标准化数据
X_scaled <- scale(X)
y_scaled <- scale(target)

# 拟合 LASSO 模型
set.seed(123)
lasso_model <- cv.glmnet(X_scaled, y_scaled, alpha = 1, nfolds = 10, standardize = TRUE)

# 提取最佳 lambda 值
best_lambda <- lasso_model$lambda.min

# 拟合模型
final_model <- glmnet(X_scaled, y_scaled, alpha = 1, lambda = 0.001)
# 提取预测值
predictions <- predict(final_model, newx = X_scaled)

# 转换为向量形式
predictions <- as.vector(predictions)

# 计算预测指标
mse <- mean((y_scaled - predictions)^2)  # 均方误差
mae <- mean(abs(y_scaled - predictions))  # 平均绝对误差
r_squared <- 1 - (sum((y_scaled - predictions)^2) / sum((y_scaled - mean(y_scaled))^2))  # 决定系数

# 打印结果
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("R-squared (R²):", r_squared, "\n")
```

#task
```{r}
# 提取因子得分
factor_scores_task <- as.data.frame(efa_result_task$scores)
factor_scores_merged <- as.data.frame(efa_result_merged$scores)
# 添加到原始数据框中
merged_with_scores_task <- cbind(factor_scores_task , factor_scores_merged)
merged_with_scores_task <- na.omit(merged_with_scores_task)
# 提取因子数据 (F1 ~ F12) 和健康指标 (第15 ~ 18列)
X <- as.matrix(merged_with_scores[, 1:5])  # 因子数据
y <- as.matrix(merged_with_scores[, 6:15])  # 健康指标

# 设置预测目标
target <- y[, 6]

# 标准化数据
X_scaled <- scale(X)
y_scaled <- scale(target)

# 拟合 LASSO 模型
set.seed(123)
lasso_model <- cv.glmnet(X_scaled, y_scaled, alpha = 1, nfolds = 10, standardize = TRUE)

# 提取最佳 lambda 值
best_lambda <- lasso_model$lambda.min

# 拟合模型
final_model <- glmnet(X_scaled, y_scaled, alpha = 1, lambda = 0.001)
# 提取预测值
predictions <- predict(final_model, newx = X_scaled)

# 转换为向量形式
predictions <- as.vector(predictions)

# 计算预测指标
mse <- mean((y_scaled - predictions)^2)  # 均方误差
mae <- mean(abs(y_scaled - predictions))  # 平均绝对误差
r_squared <- 1 - (sum((y_scaled - predictions)^2) / sum((y_scaled - mean(y_scaled))^2))  # 决定系数

# 打印结果
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("R-squared (R²):", r_squared, "\n")
```

```{r}
# 加载必要的包
library(randomForest)
library(ggplot2)
library(caret)

# 拆分数据：前11列为特征 (X)，后10列为标签 (Y)
X <- merged_with_scores[, 1:11]
Y <- merged_with_scores[, 12:21]

# 保证可重复性
set.seed(123)  # 保证可重复性

# 拆分训练集和测试集索引（70%训练，30%测试）
train_index <- createDataPartition(Y[[1]], p = 0.7, list = FALSE)

# 拆分数据
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
Y_train <- Y[train_index, 4]  
Y_test <- Y[-train_index, 4]

# 训练随机森林模型
rf_model <- randomForest(x = X_train, y = Y_train, importance = TRUE)

# 进行预测
predictions <- predict(rf_model, newdata = X_test)

# 计算残差
residuals <- Y_test - predictions

# 绘制残差图（预测值 vs. 残差）
# 构建残差数据框
residual_data <- data.frame(Predicted = predictions, Residuals = residuals)

# 绘制美化的残差图
ggplot(residual_data, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.6, color = "#0072B2", size = 2, shape = 16) +  # 蓝色点，透明度
  geom_smooth(method = "loess", se = FALSE, color = "#D55E00", size = 1) +  # 平滑线
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40", size = 0.8) +  # 零基线
  theme_minimal(base_size = 14) +  # 简洁主题
  labs(
    title = "残差图：预测值 vs 残差",
    subtitle = "用于检查随机森林模型的误差分布情况",
    x = "预测值",
    y = "残差"
  ) +
  theme(
    plot.title = element_text(face = "bold", size = 16, color = "#333333"),
    plot.subtitle = element_text(size = 12, color = "#666666"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "gray90")
  )


# 可视化变量重要性
importance_vals <- importance(rf_model)
varImpPlot(rf_model, main = "变量重要性图")

# 评估指标计算
mse <- mean((Y_test - predictions)^2)
rmse <- sqrt(mse)
mae <- mean(abs(Y_test - predictions))
r2 <- 1 - sum((Y_test - predictions)^2) / sum((Y_test - mean(Y_test))^2)

# 打印结果
cat("模型评估指标（测试集）:\n")
cat(sprintf("均方误差 (MSE): %.4f\n", mse))
cat(sprintf("均方根误差 (RMSE): %.4f\n", rmse))
cat(sprintf("平均绝对误差 (MAE): %.4f\n", mae))
cat(sprintf("R²（决定系数）: %.4f\n", r2))


```
```{r}
# 拆分数据：前11列为特征 (X)，后10列为标签 (Y)
X <- merged_with_scores_task[, 1:5]
Y <- merged_with_scores_task[, 6:15]

# 保证可重复性
set.seed(123)  

# 拆分训练集和测试集索引（70%训练，30%测试）
train_index <- createDataPartition(Y[[1]], p = 0.7, list = FALSE)

# 拆分数据
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
Y_train <- Y[train_index, 4]  
Y_test <- Y[-train_index, 4]

# 训练随机森林模型
rf_model <- randomForest(x = X_train, y = Y_train, importance = TRUE)

# 进行预测
predictions <- predict(rf_model, newdata = X_test)

# 计算残差
residuals <- Y_test - predictions

# 绘制残差图（预测值 vs. 残差）
# 构建残差数据框
residual_data <- data.frame(Predicted = predictions, Residuals = residuals)

# 绘制美化的残差图
ggplot(residual_data, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.6, color = "#0072B2", size = 2, shape = 16) +  # 蓝色点，透明度
  geom_smooth(method = "loess", se = FALSE, color = "#D55E00", size = 1) +  # 平滑线
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40", size = 0.8) +  # 零基线
  theme_minimal(base_size = 14) +  # 简洁主题
  labs(
    title = "残差图：预测值 vs 残差",
    subtitle = "用于检查随机森林模型的误差分布情况",
    x = "预测值",
    y = "残差"
  ) +
  theme(
    plot.title = element_text(face = "bold", size = 16, color = "#333333"),
    plot.subtitle = element_text(size = 12, color = "#666666"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "gray90")
  )


# 可视化变量重要性
importance_vals <- importance(rf_model)
varImpPlot(rf_model, main = "变量重要性图")
# 评估指标计算
mse <- mean((Y_test - predictions)^2)
rmse <- sqrt(mse)
mae <- mean(abs(Y_test - predictions))
r2 <- 1 - sum((Y_test - predictions)^2) / sum((Y_test - mean(Y_test))^2)

# 打印结果
cat("模型评估指标（测试集）:\n")
cat(sprintf("均方误差 (MSE): %.4f\n", mse))
cat(sprintf("均方根误差 (RMSE): %.4f\n", rmse))
cat(sprintf("平均绝对误差 (MAE): %.4f\n", mae))
cat(sprintf("R²（决定系数）: %.4f\n", r2))
```

```{r}
# 数据拆分
X <- as.matrix(merged_with_scores[, 1:11])
Y <- as.matrix(merged_with_scores[, 12:21])  # 假设是多列响应变量

y <- Y[, 10]

# 拆分训练集和测试集（70%/30%）
set.seed(123)
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

# 交叉验证找最佳 lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)

# 训练 Lasso 模型（使用最佳 lambda）
lasso_model <- glmnet(X_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min)

# 预测
predictions <- predict(lasso_model, s = cv_lasso$lambda.min, newx = X_test)

# 残差
residuals <- y_test - predictions

# 评估指标
mse <- mean(residuals^2)
rmse <- sqrt(mse)
mae <- mean(abs(residuals))
r2 <- 1 - sum(residuals^2) / sum((y_test - mean(y_test))^2)

# 打印评估结果
cat("Lasso 模型评估指标（测试集）:\n")
cat(sprintf("MSE : %.4f\n", mse))
cat(sprintf("RMSE: %.4f\n", rmse))
cat(sprintf("MAE : %.4f\n", mae))
cat(sprintf("R²  : %.4f\n", r2))


# 查看 Lasso 模型的系数（哪些变量重要）
coefficients <- coef(lasso_model)
nonzero_coefs <- coefficients[coefficients[, 1] != 0, , drop = FALSE]
cat("\nLasso 模型中被保留的重要变量（非零系数）:\n")
print(nonzero_coefs)
```


